{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f446033",
   "metadata": {},
   "source": [
    "# Details of contrastive loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533daf49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def contrastive_force(positive, negative, temp):\n",
    "    '''\n",
    "    positive: (N, D)\n",
    "    negative: (M, D)\n",
    "    temp: float\n",
    "    '''\n",
    "    N, M = positive.shape[0], negative.shape[0]\n",
    "    concat = torch.cat([negative, positive], dim=0) # [N + M, D]\n",
    "    pairwise_dist = torch.cdist(concat, concat, p=2) # [N + M, N + M]\n",
    "    affinity = torch.softmax(-pairwise_dist / temp, dim=1).pow(0.5) * \\\n",
    "                torch.softmax(-pairwise_dist / temp, dim=0).pow(0.5)  # [N + M, N + M]\n",
    "    total_force = torch.zeros_like(negative) # [M, D]\n",
    "    for j in range(M):\n",
    "        for i in range(N):\n",
    "            for k in range(M), k != j:\n",
    "                total_force[j] += affinity[j, i + M] * affinity[j, k] * (positive[i] - negative[k])\n",
    "    ... skipped normalization here. \n",
    "    return total_force\n",
    "\n",
    "def contrastive_loss(positive, negative):\n",
    "    '''\n",
    "    positive: (N, D)\n",
    "    negative: (M, D)\n",
    "    '''\n",
    "    ... skipped normalization here. \n",
    "    total_force = 0\n",
    "    for temp in [0.01, 0.02, 0.05, 0.1, 0.2]:\n",
    "        total_force += contrastive_force(positive, negative, temp)\n",
    "    target = negative + total_force # [M, D]\n",
    "    ... skipped normalization here. \n",
    "    return (negative - sg(target)) ** 2\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60279205",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def contrastive_force(positive, negative, temp):\n",
    "    '''\n",
    "    positive: (N, D)\n",
    "    negative: (M, D)\n",
    "    temp: float\n",
    "    '''\n",
    "    # main part\n",
    "    concat = torch.cat([negative, positive], dim=0) # [N + M, D]\n",
    "    pairwise_dist = torch.cdist(concat, concat, p=2) # [N + M, N + M]\n",
    "    affinity = torch.softmax(-pairwise_dist / temp, dim=1).pow(0.5) * \\\n",
    "                torch.softmax(-pairwise_dist / temp, dim=0).pow(0.5)  # [N + M, N + M]\n",
    "    total_force = torch.zeros_like(negative) # [M, D]\n",
    "    for j in range(M):\n",
    "        for i in range(N):\n",
    "            for k in range(M), k != j:\n",
    "                total_force[j] += affinity[j, i + M] * affinity[j, k] * (positive[i] - negative[k])\n",
    "    # a silly normalization below. maybe not needed. Haven't ablated. \n",
    "    total_force = total_force / ((pairwise_dist * affinity) ** 2).mean().sqrt()    \n",
    "    return total_force\n",
    "\n",
    "def contrastive_loss(positive, negative):\n",
    "    # Normalize to make sure pairwise dist has scale 1. \n",
    "    concat = torch.cat([positive, negative], dim=0) # [N + M, D]\n",
    "    pairwise_dist = torch.cdist(concat, concat, p=2) # [N + M, N + M]\n",
    "    scale = pairwise_dist.mean()\n",
    "    positive, negative = positive / scale, negative / scale\n",
    "    # main part\n",
    "    total_force = 0\n",
    "    for temp in [0.01, 0.02, 0.05, 0.1, 0.2]:\n",
    "        total_force += contrastive_force(positive, negative, temp)\n",
    "    target = negative + total_force # [M, D]\n",
    "    # Remark: in actual implementation, normalized [B, M, D] to have scale 1. \n",
    "    return (negative - sg(target)) ** 2\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
